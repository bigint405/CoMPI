import gc
import onnxruntime as ort
import torch
import numpy as np
import pynvml
import threading
import time
import nvtx

def get_onnx_running_data(model_path, num_instances=1, batchsize=1):
    # **8ï¸âƒ£ é‡Šæ”¾ GPU èµ„æº**
    # print("ğŸ§¹ Cleaning up GPU memory...")
    # torch.cuda.empty_cache()  # **æ¸…ç©º CUDA ç¼“å­˜**
    # torch.cuda.synchronize()  # **ç¡®ä¿æ¸…ç†å®Œæˆ**

    # **åˆå§‹åŒ– pynvml**
    pynvml.nvmlInit()
    device_id = 0  # **ç›‘æµ‹çš„ GPU**
    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)

    # **ç›‘æµ‹ GPU æ˜¾å­˜**
    def get_gpu_memory():
        """è·å–å½“å‰ GPU æ˜¾å­˜å ç”¨ï¼ˆBï¼ŒMBï¼‰"""
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        return info.used, info.free  # **å•ä½ï¼šBï¼ˆå­—èŠ‚ï¼‰**

    # **å®æ—¶ç›‘æµ‹æ¨ç†è¿‡ç¨‹ä¸­çš„æœ€å¤§æ˜¾å­˜**
    def monitor_gpu_usage(stop_event, max_mem_list):
        """GPU ç›‘æµ‹çº¿ç¨‹ï¼šæŒç»­è®°å½•æœ€å¤§æ˜¾å­˜å ç”¨"""
        max_mem_usage = 0
        while not stop_event.is_set():
            current_mem, _ = get_gpu_memory()
            max_mem_usage = max(max_mem_usage, current_mem)
            time.sleep(0.01)  # **10ms è½®è¯¢ä¸€æ¬¡**
        max_mem_list.append(max_mem_usage)  # **å­˜å‚¨æœ€ç»ˆçš„æœ€å¤§å€¼**

    # **ONNX è¿è¡Œè®¾å¤‡**
    providers = [("CUDAExecutionProvider", {"device_id": device_id, "arena_extend_strategy": "kSameAsRequested"})]
    # providers = [("CUDAExecutionProvider", {"device_id": device_id, "arena_extend_strategy": "kSameAsRequested"})]

    # **1ï¸âƒ£ ç›‘æµ‹æ¨¡å‹åŠ è½½æ˜¾å­˜**
    mem_before_load, free_before_load = get_gpu_memory()
    print(f"ğŸ“Œ GPU memory used before load: {mem_before_load} B ({mem_before_load / 1024**2:.2f} MB)")
    print(f"ğŸ“Œ GPU free memory before load: {free_before_load} B ({free_before_load / 1024**2:.2f} MB)")
    
    nvtx.push_range('load_model')
    sessions = [ort.InferenceSession(model_path, providers=providers) for _ in range(num_instances)]
    torch.cuda.synchronize()
    mem_after_load, _ = get_gpu_memory()
    model_gpu_memory = mem_after_load - mem_before_load
    print(f"ğŸ“Œ {num_instances} models loaded. Total GPU memory used: {model_gpu_memory / 1024**2:.2f} MB")
    nvtx.pop_range()

    # **2ï¸âƒ£ è§£æ ONNX è¾“å…¥ shapeï¼Œå¹¶è®¾ç½® batch_size**
    batch_size = batchsize  # **æ‰‹åŠ¨è®¾ç½® batch_size**
    input_shapes = {}
    input_datas = {}
    input_data_size = 0

    nvtx.push_range('load_data_to_cpu')
    for input_tensor in sessions[0].get_inputs():
        name = input_tensor.name
        shape = list(input_tensor.shape)

        # **åŠ¨æ€æ›¿æ¢ batch ç»´åº¦ï¼ˆå¦‚æœæ˜¯ Noneï¼Œåˆ™æ›¿æ¢ä¸º batch_sizeï¼‰**
        shape[0] = batch_size if shape[0] is None or type(shape[0]) != int or shape[0]<=0 else shape[0]
        input_shapes[name] = tuple(shape)
        d = np.random.randn(*shape).astype(np.float32)
        input_datas[name] = d
        input_data_size += d.nbytes

    print(f"ğŸ“Œ ONNX Input Shapes: {input_shapes}, Numpy Tensor Size: {input_data_size} B, {input_data_size / 1024**2} MB")
    nvtx.pop_range()

    # **3ï¸âƒ£ ç›‘æµ‹è¾“å…¥æ•°æ®åŠ è½½æ˜¾å­˜**
    nvtx.push_range('load_data_to_gpu')
    torch.cuda.synchronize()
    mem_before_input, _ = get_gpu_memory()

    inputs = {
        name: ort.OrtValue.ortvalue_from_numpy(data, device_type="cuda", device_id=device_id)
        for name, data in input_datas.items()
    }
    inputs_list = [inputs] * num_instances
    nvtx.pop_range()

    torch.cuda.synchronize()
    time.sleep(3)
    mem_after_input, _ = get_gpu_memory()
    input_gpu_memory = mem_after_input - mem_before_input
    print(f"ğŸ“Œ Input loaded. GPU memory used: {input_gpu_memory} B ({input_gpu_memory / 1024**2:.2f} MB)")

    # **4ï¸âƒ£ å¯åŠ¨ç›‘æµ‹çº¿ç¨‹**
    stop_event = threading.Event()  # **ç”¨ Event æ§åˆ¶çº¿ç¨‹ç»“æŸ**
    max_mem_list = []  # **å­˜å‚¨æœ€å¤§æ˜¾å­˜**
    
    monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(stop_event, max_mem_list), daemon=True)
    monitor_thread.start()

    # **5ï¸âƒ£ è¿è¡Œ ONNX æ¨ç†**
    mem_before_infer, _ = get_gpu_memory()
    nvtx.push_range('infer')
    for _ in range(3):
        for session in sessions:
            output = session.run(None, inputs)
            del output  # âœ… ç«‹å³æ¸…é™¤æ¨ç†è¾“å‡ºï¼Œé˜²æ­¢å ç”¨ç´¯ç§¯
        time.sleep(1)

    torch.cuda.synchronize()
    nvtx.pop_range()

    # **6ï¸âƒ£ åœæ­¢ç›‘æµ‹çº¿ç¨‹**
    stop_event.set()
    monitor_thread.join()

    # **7ï¸âƒ£ è®¡ç®—æ¨ç†æœŸé—´çš„æœ€å¤§æ˜¾å­˜**
    max_mem_usage = max(max_mem_list)
    peak_memory_during_inference = max_mem_usage - mem_before_infer
    print(f"ğŸ“Œ Peak GPU memory during inference: {peak_memory_during_inference} B ({peak_memory_during_inference / 1024**2:.2f} MB, total before load {(max_mem_usage - mem_before_load) / 1024**2:.2f} MB)")

    # **8ï¸âƒ£ é‡Šæ”¾ GPU èµ„æº**
    print("ğŸ§¹ Cleaning up GPU memory...")

    for session in sessions:
        session.set_providers([])
        del session
    del sessions

    for k in list(inputs.keys()):
        del inputs[k]
    del inputs  # âœ… æ¸…ç†å”¯ä¸€ä¸€ä»½å…±äº« OrtValue

    gc.collect()
    torch.cuda.empty_cache()  # **æ¸…ç©º CUDA ç¼“å­˜**
    torch.cuda.synchronize()  # **ç¡®ä¿æ¸…ç†å®Œæˆ**

    # **9ï¸âƒ£ å†æ¬¡æ£€æŸ¥ GPU æ˜¾å­˜**
    mem_after_cleanup, free_after_cleanup = get_gpu_memory()
    print(f"ğŸ“Œ GPU memory after cleanup: {mem_after_cleanup} B ({mem_after_cleanup / 1024**2:.2f} MB)")
    print(f"ğŸ“Œ GPU free memory after cleanup: {free_after_cleanup} B ({free_after_cleanup / 1024**2:.2f} MB)")

if __name__ == '__main__':
    model_path = ["/workspace/datas/models/resnet152-1000/resnet152.onnx"]
    # model_path = ["/workspace/co-mpi/test/models_seg/part0.onnx", "/workspace/co-mpi/test/models_seg/part1.onnx", "/workspace/datas/models/resnet152-1000/resnet152.onnx"]
    bs = [1, 2, 4, 8, 16]
    for p in model_path:
        for b in bs:
            for n in [1, 1, 16]:
                print(f"ğŸ“Œ Running {n} instances of Model: {p}, batchsize={b}")
                get_onnx_running_data(p, num_instances=n, batchsize=b)
                print("-" * 60)
                time.sleep(3)

